# -*- coding: utf-8 -*-
"""parser_nvkonline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qixHnw-QQ_MqHOcGNw1WMs8Ev-3ERdd-
"""

import requests
from bs4 import BeautifulSoup

def find_all_urls(url):
    page = requests.get(url) 
    try:
        soup = BeautifulSoup(page.text, "html.parser")
        urls_articles = []
        parts = soup.find_all("h2", {"class": "entry-title"})
        for part in parts:
              part = str(part).split('<a href="')[1]
              part = str(part).split('">')[0]
              urls_articles += [part]
        return urls_articles
    except Exception as exc:
        return []


address = 'http://nvk-online.ru/new/category/glavnye-novosti/'
list_of_urls = []
for i in range(1,630):
    if i==1:
        nextpage = find_all_urls(address)
    else: 
        nextpage = find_all_urls(address + 'page/'+str(i)+'/')
    if nextpage!=[]:
        list_of_urls += nextpage
    else:    
        break          
textfile = "sakha_nvkonline_urls.txt"
with open(textfile, 'w') as f:
    f.write("\n".join(list_of_urls))      

with open(textfile, 'r') as f:
    mystring = f.read()
list_of_urls2 = mystring.split("\n")   

raw_data_list = []
for url in list_of_urls2:
    cur_article = {}
    page = requests.get(url)
    sp = BeautifulSoup(page.content, 'html.parser')
    cur_article['url'] = url
    cur_article['raw_text'] = [_.get_text().replace("\xa0", " ") for _ in sp.find_all('p')][:-2]
    raw_data_list.append(cur_article)

import json
with open('raw_data_nvkonline.json', 'w') as outfile:
    json.dump(raw_data_list, outfile, ensure_ascii=False)





